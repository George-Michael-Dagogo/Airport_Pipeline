{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating snowflake connection and pushing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/28 17:52:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2986043, Pic de Font Blanca, Pic de Font Blanca, Pic de Font Blanca,Pic du Port, 42.64991, 1.53335, T, PK, AD, , 00, , , , 0, , 2860, Europe/Andorra, 2014-11-05\n",
      " Schema: 2986043, Pic de Font Blanca1, Pic de Font Blanca2, Pic de Font Blanca,Pic du Port, 42.64991, 1.53335, T, PK, AD, _c9, 00, _c11, _c12, _c13, 0, _c15, 2860, Europe/Andorra, 2014-11-05\n",
      "Expected: Pic de Font Blanca1 but found: Pic de Font Blanca\n",
      "CSV file: file:///workspace/Airport_Pipeline/Geonames_data/allCountries.txt\n",
      "23/05/28 17:52:35 ERROR Executor: Exception in task 7.0 in stage 11.0 (TID 18)6]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2699/0x0000000101265040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2691/0x000000010125c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2688/0x000000010124b040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2649/0x0000000101221840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/05/28 17:52:36 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 11.0 (TID 18),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2699/0x0000000101265040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2691/0x000000010125c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2688/0x000000010124b040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2649/0x0000000101221840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 7.0 in stage 11.0 (TID 18) (10.0.5.2 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2699/0x0000000101265040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2691/0x000000010125c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2688/0x000000010124b040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2649/0x0000000101221840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/05/28 17:52:36 ERROR TaskSetManager: Task 7 in stage 11.0 failed 1 times; aborting job\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 4.0 in stage 11.0 (TID 15) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 14.0 in stage 11.0 (TID 25) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 12.0 in stage 11.0 (TID 23) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 15.0 in stage 11.0 (TID 26) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 9.0 in stage 11.0 (TID 20) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 13.0 in stage 11.0 (TID 24) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 11.0 in stage 11.0 (TID 22) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 1.0 in stage 11.0 (TID 12) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 2.0 in stage 11.0 (TID 13) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 11) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 10.0 in stage 11.0 (TID 21) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 8.0 in stage 11.0 (TID 19) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 6.0 in stage 11.0 (TID 17) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 3.0 in stage 11.0 (TID 14) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/28 17:52:36 WARN TaskSetManager: Lost task 5.0 in stage 11.0 (TID 16) (10.0.5.2 executor driver): TaskKilled (Stage cancelled)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    148\u001b[0m     if c is not None and (\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mis_instance_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.spark.api.python.PythonException\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;31m# To make sure this only catches Python UDFs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return gateway.jvm.py4j.reflection.TypeUtil.isInstanceOf(\n\u001b[0m\u001b[1;32m    465\u001b[0m         param, java_object)\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1724\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1672/2519166152.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mheader_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Specify the target table name in Snowflake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geonames'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Tom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/pyspark/traceback_utils.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "engine = create_engine(\n",
    "    'snowflake://{user}:{password}@{account}/'.format(\n",
    "        user='GEORGE9042',\n",
    "        password='George9042',\n",
    "        account='plninim-tg58176',\n",
    "        warehouse='COMPUTE_WH',\n",
    "        database='MY_TEST', \n",
    "        schema='NEW'\n",
    "    )\n",
    ")\n",
    "sfOptions = {\n",
    "    \"sfURL\": \"https://lv50176.switzerland-north.azure.snowflakecomputing.com\",\n",
    "    \"sfAccount\": \"plninim-tg58176\",\n",
    "    \"sfUser\": \"GEORGE9042\",\n",
    "    \"sfPassword\": \"George9042\",\n",
    "    \"sfDatabase\": \"MY_TEST\",\n",
    "    \"sfSchema\": \"NEW\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "}\n",
    "try:\n",
    "    connection = engine.connect()\n",
    "    connection.execute('USE ROLE ACCOUNTADMIN')\n",
    "    connection.execute('USE DATABASE MY_TEST')\n",
    "    connection.execute('USE SCHEMA NEW')\n",
    "\n",
    "    #airports = pd.read_csv('/workspace/Airport_Pipeline/Airport_data/airports.csv')\n",
    "    #airports.to_sql('airports', con=connection, if_exists='replace',index = False,chunksize=16000)   \n",
    "\n",
    "    #airport_frequencies = pd.read_csv('/workspace/Airport_Pipeline/Airport_data/airport-frequencies.csv')\n",
    "    #airport_frequencies.to_sql('airport_frequencies', con=connection, if_exists='replace',index = False,chunksize=16000) \n",
    "\n",
    "    #countries = pd.read_csv('/workspace/Airport_Pipeline/Airport_data/countries.csv')\n",
    "    #countries.to_sql('countries', con=connection, if_exists='replace',index = False,chunksize=16000)\n",
    "\n",
    "    #airport_comments = pd.read_csv('/workspace/Airport_Pipeline/Airport_data/airport-comments.csv')\n",
    "    #airport_comments.to_sql('airport_comments', con=connection, if_exists='replace',index = False,chunksize=16000)\n",
    "\n",
    "    #navaids = pd.read_csv('/workspace/Airport_Pipeline/Airport_data/navaids.csv')\n",
    "    #navaids.to_sql('navaids', con=connection, if_exists='replace',index = False,chunksize=16000)\n",
    "\n",
    "    #regions = pd.read_csv('/workspace/Airport_Pipeline/Airport_data/regions.csv')\n",
    "    #regions.to_sql('regions', con=connection, if_exists='replace',index = False,chunksize=16000)\n",
    "\n",
    "    #runways = pd.read_csv('/workspace/Airport_Pipeline/Airport_data/runways.csv')\n",
    "    #runways.to_sql('runways', con=connection, if_exists='replace',index = False,chunksize=16000)\n",
    "\n",
    "#using Pyspark because the table has 12 million rows\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"ReadTextFile\").getOrCreate()\n",
    "    header_names = [\"geoname_id\", \"name\", \"asciiname\",\"alternatenames\",\"latitude\",\"longitude\",\n",
    "    \"feature_class\",\"feature_code\",\"country_code\",\"altcountry_code\",\"admin1_code\",\"admin2_code\"\n",
    "    ,\"admin3_code\",\"admin4_code\",\"population\",\"elevation\",\"DEM\",\"timezone\",\"modification_date\" ]\n",
    "    path = \"/workspace/Airport_Pipeline/Geonames_data/allCountries.txt\"\n",
    "\n",
    "\n",
    "#text_file = spark.read.format(\"csv\").option(\"delimiter\", \"\\t\").load(\"./Geonames_data/allCountries.txt\")\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").csv(path).toDF(*header_names)\n",
    "    # Specify the target table name in Snowflake\n",
    "    pdf = df.toPandas()\n",
    "    pdf.to_sql('geonames', con=connection, if_exists='replace',index = False,chunksize=16000)\n",
    "\n",
    "finally:\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many airports, airfields and heliports exist in each country and continent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AN', 5, 9, 25)\n",
      "('EU', 1065, 1714, 5679)\n",
      "('SA', 434, 2152, 7312)\n",
      "('NA', 2487, 8805, 17931)\n",
      "('AF', 517, 209, 3056)\n",
      "('AS', 1483, 5712, 2587)\n",
      "('OC', 353, 479, 2829)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "engine = create_engine(\n",
    "    'snowflake://{user}:{password}@{account}/'.format(\n",
    "        user='GEORGE9042',\n",
    "        password='George9042',\n",
    "        account='plninim-tg58176',\n",
    "        warehouse='COMPUTE_WH',\n",
    "        database='MY_TEST', \n",
    "        schema='NEW'\n",
    "    )\n",
    ")\n",
    "try:\n",
    "    connection = engine.connect()\n",
    "    connection.execute('USE ROLE ACCOUNTADMIN')\n",
    "    connection.execute('USE DATABASE MY_TEST')\n",
    "    connection.execute('USE SCHEMA NEW')\n",
    "    clean_continent = '''UPDATE airports SET CONTINENT = COALESCE(CONTINENT, 'NA')'''\n",
    "    clean_country = '''UPDATE airports SET iso_country = COALESCE(iso_country, 'NA')'''\n",
    "\n",
    "    query_continent = '''select * from airport_per_continent'''\n",
    "\n",
    "    query_country='''select * from airport_per_country '''\n",
    "    result = connection.execute(query_continent)\n",
    "    for i in result:\n",
    "             print(i)\n",
    "finally:\n",
    "    connection.close()\n",
    "    engine.dispose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Snowflake Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create or replace TABLE MY_TEST.NEW.AIRPORT_COMMENTS (\n",
    "\tID NUMBER(38,0),\n",
    "\tthreadRef FLOAT,\n",
    "\tairportRef NUMBER(38,0),\n",
    "\tairportIdent VARCHAR(16777216),\n",
    "\tdate VARCHAR(16777216),\n",
    "\tmemberNickname VARCHAR(16777216),\n",
    "\tsubject VARCHAR(16777216),\n",
    "\tbody VARCHAR(16777216),\n",
    "    PRIMARY KEY (ID)\n",
    ");\n",
    "\n",
    "create or replace TABLE MY_TEST.NEW.AIRPORTS (\n",
    "\tID NUMBER(38,0) NOT NULL,\n",
    "\tIDENT VARCHAR(16777216),\n",
    "\tTYPE VARCHAR(16777216),\n",
    "\tNAME VARCHAR(16777216),\n",
    "\tLATITUDE_DEG FLOAT,\n",
    "\tLONGITUDE_DEG FLOAT,\n",
    "\tELEVATION_FT NUMBER(38,0),\n",
    "\tCONTINENT VARCHAR(16777216),\n",
    "\tISO_COUNTRY VARCHAR(16777216),\n",
    "\tISO_REGION VARCHAR(16777216),\n",
    "\tMUNICIPALITY VARCHAR(16777216),\n",
    "\tSCHEDULED_SERVICE VARCHAR(16777216),\n",
    "\tGPS_CODE VARCHAR(16777216),\n",
    "\tIATA_CODE VARCHAR(16777216),\n",
    "\tLOCAL_CODE VARCHAR(16777216),\n",
    "\tHOME_LINK VARCHAR(16777216),\n",
    "\tWIKIPEDIA_LINK VARCHAR(16777216),\n",
    "\tKEYWORDS VARCHAR(16777216),\n",
    "\tprimary key (ID),\n",
    "\tforeign key (ISO_REGION) references MY_TEST.NEW.REGIONS(CODE)\n",
    ");\n",
    "\n",
    "create or replace TABLE MY_TEST.NEW.RUNWAYS (\n",
    "\tID NUMBER(38,0) NOT NULL,\n",
    "\tAIRPORT_REF NUMBER(38,0),\n",
    "\tAIRPORT_IDENT VARCHAR(16777216),\n",
    "\tLENGTH_FT FLOAT,\n",
    "\tWIDTH_FT FLOAT,\n",
    "\tSURFACE VARCHAR(16777216),\n",
    "\tLIGHTED BOOLEAN,\n",
    "\tCLOSED BOOLEAN,\n",
    "\tLE_IDENT VARCHAR(16777216),\n",
    "\tLE_LATITUDE_DEG FLOAT,\n",
    "\tLE_LONGITUDE_DEG FLOAT,\n",
    "\tLE_ELEVATION_FT NUMBER(38,0),\n",
    "\tLE_HEADING_DEGT FLOAT,\n",
    "\tLE_DISPLACED_THRESHOLD_FT NUMBER(38,0),\n",
    "\tHE_IDENT VARCHAR(16777216),\n",
    "\tHE_LATITUDE_DEG FLOAT,\n",
    "\tHE_LONGITUDE_DEG FLOAT,\n",
    "\tHE_ELEVATION_FT NUMBER(38,0),\n",
    "\tHE_HEADING_DEGT FLOAT,\n",
    "\tHE_DISPLACED_THRESHOLD_FT NUMBER(38,0),\n",
    "\tprimary key (ID),\n",
    "\tforeign key (AIRPORT_REF) references MY_TEST.NEW.AIRPORTS(ID)\n",
    ");\n",
    "\n",
    "create or replace TABLE MY_TEST.NEW.REGIONS (\n",
    "\tCODE VARCHAR(16777216) NOT NULL,\n",
    "\tLOCAL_CODE VARCHAR(16777216),\n",
    "\tISO_COUNTRY VARCHAR(16777216),\n",
    "\tNAME VARCHAR(16777216),\n",
    "\tCONTINENT VARCHAR(16777216),\n",
    "\tprimary key (CODE)\n",
    ");\n",
    "\n",
    "create or replace TABLE MY_TEST.NEW.NAVAIDS (\n",
    "\tID_NAV NUMBER(38,0) NOT NULL,\n",
    "\tFILENAME VARCHAR(16777216),\n",
    "\tIDENT VARCHAR(16777216),\n",
    "\tNAME VARCHAR(16777216),\n",
    "\tTYPE VARCHAR(16777216),\n",
    "\tFREQUENCY_KHZ FLOAT,\n",
    "\tLATITUDE_DEG FLOAT,\n",
    "\tLONGITUDE_DEG FLOAT,\n",
    "\tELEVATION_FT NUMBER(38,0),\n",
    "\tISO_COUNTRY VARCHAR(16777216),\n",
    "\tDME_FREQUENCY_KHZ FLOAT,\n",
    "\tDME_CHANNEL NUMBER(38,0),\n",
    "\tDME_LATITUDE_DEG FLOAT,\n",
    "\tDME_LONGITUDE_DEG FLOAT,\n",
    "\tDME_ELEVATION_FT NUMBER(38,0),\n",
    "\tSLAVED_VARIATION_DEG FLOAT,\n",
    "\tMAGNETIC_VARIATION_DEG FLOAT,\n",
    "\tUSAGE_TYPE VARCHAR(16777216),\n",
    "\tPOWER FLOAT,\n",
    "\tASSOCIATED_AIRPORT VARCHAR(16777216),\n",
    "\tASSOCIATED_AIRPORT_IDENT VARCHAR(16777216),\n",
    "\tprimary key (ID_NAV)\n",
    ");\n",
    "\n",
    "create or replace TABLE MY_TEST.NEW.COUNTRIES (\n",
    "\tID NUMBER(38,0),\n",
    "\tCODE VARCHAR(16777216),\n",
    "\tNAME VARCHAR(16777216),\n",
    "\tCONTINENT VARCHAR(16777216),\n",
    "\tWIKIPEDIA_LINK VARCHAR(16777216),\n",
    "\tKEYWORDS VARCHAR(16777216)\n",
    "\tprimary key (ID)\n",
    ");\n",
    "\n",
    "create or replace TABLE MY_TEST.NEW.AIRPORT_COMMENTS (\n",
    "\tID NUMBER(38,0),\n",
    "\tthreadRef FLOAT,\n",
    "\tairportRef NUMBER(38,0),\n",
    "\tairportIdent VARCHAR(16777216),\n",
    "\tDATE VARCHAR(16777216),\n",
    "\tmemberNickname VARCHAR(16777216),\n",
    "\tSUBJECT VARCHAR(16777216),\n",
    "\tBODY VARCHAR(16777216)\\\n",
    "\tPRIMARY KEY (ID)\n",
    ");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views used in snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_per_continent = '''create or replace view AIRPORT_PER_CONTINENT as\n",
    "    -- comment = '<comment>'\n",
    "    SELECT continent,    \n",
    "       COUNT(CASE WHEN type IN ('seaplane_base','medium_airport', 'large_airport') THEN 1 END) AS airports,\n",
    "       COUNT(CASE WHEN type = 'heliport' THEN 1 END) AS heliport,\n",
    "       COUNT(CASE WHEN type = 'small_airport' THEN 1 END) AS airfield       \n",
    "       FROM airports\n",
    "       GROUP BY continent;'''\n",
    "\n",
    "airport_per_country = '''create or replace view AIRPORT_PER_COUNTRY as\n",
    "SELECT iso_country,\n",
    "/* assumumes the small airport is an airfield*/\n",
    "       COUNT(CASE WHEN type IN ('seaplane_base','medium_airport', 'large_airport') THEN 1 END) AS airports,\n",
    "       COUNT(CASE WHEN type = 'heliport' THEN 1 END) AS heliport,\n",
    "       COUNT(CASE WHEN type = 'small_airport' THEN 1 END) AS airfield       \n",
    "       FROM airports\n",
    "       GROUP BY iso_country;'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Business questions with PostgreSQL\n",
    "1. \n",
    "\n",
    "SELECT c.name AS country, \n",
    "       COUNT(CASE WHEN a.type = 'large_airport' THEN 1 END) AS airports,\n",
    "       COUNT(CASE WHEN a.type = 'small_airport' THEN 1 END) AS airfields,\n",
    "       COUNT(CASE WHEN a.type = 'heliport' THEN 1 END) AS heliports\n",
    "FROM countries c\n",
    "JOIN regions r ON c.code = r.iso_country\n",
    "JOIN airports a ON r.code = a.iso_region\n",
    "GROUP BY c.name;\n",
    "// BY CONTINENT\n",
    "\n",
    "SELECT a.continent,\n",
    "       COUNT(CASE WHEN a.type = 'large_airport' THEN 1 END) AS airports,\n",
    "       COUNT(CASE WHEN a.type = 'small_airport' THEN 1 END) AS airfields,\n",
    "       COUNT(CASE WHEN a.type = 'heliport' THEN 1 END) AS heliports\n",
    "FROM airports a\n",
    "GROUP BY a.continent;\n",
    "\n",
    "2. \n",
    "SELECT c.name AS country,\n",
    "       AVG(CASE WHEN a.type = 'large_airport' THEN a.elevation_ft END) AS avg_airport_elevation,\n",
    "       AVG(CASE WHEN a.type = 'small_airport' THEN a.elevation_ft END) AS avg_airfield_elevation,\n",
    "       AVG(CASE WHEN a.type = 'heliport' THEN a.elevation_ft END) AS avg_heliport_elevation\n",
    "FROM countries c\n",
    "JOIN regions r ON c.code = r.iso_country\n",
    "JOIN airports a ON r.code = a.iso_region\n",
    "GROUP BY c.name;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
